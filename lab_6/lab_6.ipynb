{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer: np.ndarray = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "(143, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Makes the data compatible with the models\n",
    "- `MinMaxScaler` - Transform features by scaling each feature to a given range\n",
    "  - This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler: MinMaxScaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `fit()` computes the minimum and maximum value of each feature of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `transform()` method applies the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled: np.ndarray = scaler.transform(X_train) # transform training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "[6.981e+00 9.710e+00 4.379e+01 1.435e+02 5.263e-02 1.938e-02 0.000e+00\n",
      " 0.000e+00 1.060e-01 4.996e-02 1.115e-01 3.628e-01 7.570e-01 7.228e+00\n",
      " 1.713e-03 2.252e-03 0.000e+00 0.000e+00 7.882e-03 8.948e-04 7.930e+00\n",
      " 1.202e+01 5.041e+01 1.852e+02 7.117e-02 2.729e-02 0.000e+00 0.000e+00\n",
      " 1.565e-01 5.504e-02]\n",
      "[2.811e+01 3.381e+01 1.885e+02 2.501e+03 1.447e-01 3.114e-01 4.268e-01\n",
      " 2.012e-01 3.040e-01 9.744e-02 2.873e+00 4.885e+00 2.198e+01 5.422e+02\n",
      " 2.333e-02 1.064e-01 3.960e-01 5.279e-02 6.146e-02 2.984e-02 3.604e+01\n",
      " 4.954e+01 2.512e+02 4.254e+03 2.226e-01 1.058e+00 1.252e+00 2.903e-01\n",
      " 6.638e-01 2.075e-01]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled.shape)\n",
    "print(X_train.min(axis=0)) # min value per feature before scaling\n",
    "print(X_train.max(axis=0)) # max value per feature before scaling\n",
    "print(X_train_scaled.min(axis=0)) # min value per feature after scaling\n",
    "print(X_train_scaled.max(axis=0)) # max value per feature after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled: np.ndarray = scaler.transform(X_test) # transform test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 30)\n",
      "[7.729e+00 1.072e+01 4.798e+01 1.788e+02 6.576e-02 3.398e-02 0.000e+00\n",
      " 0.000e+00 1.203e-01 5.024e-02 1.144e-01 3.602e-01 7.714e-01 6.802e+00\n",
      " 2.826e-03 3.746e-03 0.000e+00 0.000e+00 1.013e-02 1.217e-03 8.964e+00\n",
      " 1.249e+01 5.717e+01 2.422e+02 8.409e-02 4.619e-02 0.000e+00 0.000e+00\n",
      " 1.603e-01 5.865e-02]\n",
      "[2.321e+01 3.928e+01 1.535e+02 1.670e+03 1.634e-01 3.454e-01 4.264e-01\n",
      " 1.823e-01 2.906e-01 9.502e-02 1.370e+00 3.647e+00 1.107e+01 1.765e+02\n",
      " 3.113e-02 1.354e-01 1.438e-01 4.090e-02 7.895e-02 2.193e-02 3.101e+01\n",
      " 4.487e+01 2.068e+02 2.944e+03 1.902e-01 9.327e-01 1.170e+00 2.910e-01\n",
      " 5.440e-01 1.446e-01]\n",
      "[ 0.03540158  0.04190871  0.02895446  0.01497349  0.14260888  0.04999658\n",
      "  0.          0.          0.07222222  0.00589722  0.00105015 -0.00057494\n",
      "  0.00067851 -0.0007963   0.05148726  0.01434497  0.          0.\n",
      "  0.04195752  0.01113138  0.03678406  0.01252665  0.03366702  0.01400904\n",
      "  0.08531995  0.01833687  0.          0.          0.00749064  0.02367834]\n",
      "[0.76809125 1.22697095 0.75813696 0.64750795 1.20310633 1.11643038\n",
      " 0.99906279 0.90606362 0.93232323 0.94903117 0.45573058 0.72623944\n",
      " 0.48593507 0.31641282 1.36082713 1.2784499  0.36313131 0.77476795\n",
      " 1.32643996 0.72672498 0.82106012 0.87553305 0.77887345 0.67803775\n",
      " 0.78603975 0.87843331 0.93450479 1.0024113  0.76384782 0.58743277]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_scaled.shape)\n",
    "print(X_test.min(axis=0)) # min value per feature before scaling\n",
    "print(X_test.max(axis=0)) # max value per feature before scaling\n",
    "print(X_test_scaled.min(axis=0)) # min value per feature after scaling\n",
    "print(X_test_scaled.max(axis=0)) # max value per feature after scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the test set, after scaling, the minimum and maximum are not 0 and 1\n",
    "- `MinMaxScaler` (and all the other scalers) always applies exactly the same transformation to the training and the test set\n",
    "- This means the transform method always subtracts the training set minimum and divides by the training set range, which might be different from the minimum and range for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.000\n"
     ]
    }
   ],
   "source": [
    "svm: SVC = SVC(C=100)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(svm.score(X_train_scaled, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 1.000\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled: np.ndarray = scaler.transform(X_test)\n",
    "svm.fit(X_test_scaled, y_test)\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svm.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Selection using Validation & Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris: np.ndarray = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `SVM()` takes 2 arguments \n",
    "  - `gamma` - kernel bandwidth \n",
    "  - `C` - regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score: float = 0\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]: # try different values for gamma\n",
    "\tfor C in [0.001, 0.01, 0.1, 1, 10, 100]: # try different values for C\n",
    "\t\tsvm: SVC = SVC(gamma=gamma, C=C) # build the model\n",
    "\t\tsvm.fit(X_train, y_train) # train the model\n",
    "\t\tscore: float = svm.score(X_test, y_test) # evaluate the model on the test set\n",
    "\t\tif score > best_score: # if we got a better score, store the score and parameters\n",
    "\t\t\tbest_score = score # store the best score\n",
    "\t\t\tbest_parameters = {'C': C, 'gamma': gamma} # store the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.97\n",
      "Best parameters: {'C': 100, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: {}\".format(best_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Repeating same procedure as before but with a validation set\n",
    "- Training set is split into 2 parts, the training set and the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0) # split the data into training and test sets\n",
    "X_train_pr, X_valid, y_train_pr, y_valid = train_test_split(X_train, y_train, random_state=1) # split training set into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 84 \n",
      "Size of validation set: 28 \n",
      "Size of test set: 38\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training set: {} \\nSize of validation set: {} \\nSize of test set: {}\".format(X_train_pr.shape[0], X_valid.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score: float = 0\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]: # try different values for gamma\n",
    "\tfor C in [0.001, 0.01, 0.1, 1, 10, 100]: # try different values for C\n",
    "\t\tsvm: SVC = SVC(gamma=gamma, C=C) # build the model \n",
    "\t\tsvm.fit(X_train_pr, y_train_pr) # train the model on the training set \n",
    "\t\tscore: float = svm.score(X_valid, y_valid) # evaluate the model on the validation set\n",
    "\t\tif score > best_score: # if we got a better score, store the score and parameters\n",
    "\t\t\tbest_score = score # store the best score\n",
    "\t\t\tbest_parameters = {'C': C, 'gamma': gamma} # store best parameters (will use **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm: SVC = SVC(**best_parameters) # build a model with best parameters (**kwargs)\n",
    "svm.fit(X_train, y_train) # fit the model using the whole training set\n",
    "test_score: float = svm.score(X_test, y_test) # evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score on validation set: 0.96\n",
      "Best parameters: {'C': 10, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score on validation set: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: {}\".format(best_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "\tfor C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "\t\tsvm: SVC = SVC(gamma=gamma, C=C)\n",
    "\t\tscore: float = np.mean(cross_val_score(svm, X_train, y_train, cv=5))\n",
    "\t\tif score > best_score:\n",
    "\t\t\tbest_score = score\n",
    "\t\t\tbest_C = C\n",
    "\t\t\tbest_gamma = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=best_C, gamma=best_gamma)\n",
    "svm.fit(X_train, y_train)\n",
    "test_score = svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.97\n",
      "Best parameters: C = 10, gamma = 0.1\n",
      "Test set score with best parameters: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Best cross-validation score: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: C = {}, gamma = {}\".format(best_C, best_gamma))\n",
    "print(\"Test set score with best parameters: {:.2f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `GridSearchCV` implements the grid search with-cross validation\n",
    "- It will perform all the necessary model fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A dictionary is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid: dict[str, list[float]] = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Behaves similarly to a classifier as it can call:\n",
    "  - `fit`\n",
    "  - `predict`\n",
    "  - `score`\n",
    "- However, when calling `fit`, it will run cross-validation for each combination of parameters which was specified in `param_grid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search: GridSearchCV = GridSearchCV(SVC(), param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data still needs to be split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTest set score: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(grid_search\u001b[39m.\u001b[39;49mscore(X_test, y_test)))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/anaconda-pOKeClin-py3.10/lib64/python3.10/site-packages/sklearn/model_selection/_search.py:437\u001b[0m, in \u001b[0;36mBaseSearchCV.score\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39m\"\"\"Return the score on the given data, if the estimator has been refit.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \n\u001b[1;32m    416\u001b[0m \u001b[39mThis uses the score defined by ``scoring`` where provided, and the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39m    ``best_estimator_.score`` method otherwise.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m _check_refit(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 437\u001b[0m check_is_fitted(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscorer_ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo score function explicitly defined, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mand the estimator doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt provide one \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m         \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\n\u001b[1;32m    443\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/anaconda-pOKeClin-py3.10/lib64/python3.10/site-packages/sklearn/utils/validation.py:1345\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     fitted \u001b[39m=\u001b[39m [\n\u001b[1;32m   1341\u001b[0m         v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mvars\u001b[39m(estimator) \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m v\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1342\u001b[0m     ]\n\u001b[1;32m   1344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'gamma': 0.1}\n",
      "Best cross-validation score: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, gamma=0.1)\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revision Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "What is meant by data normalization in machine learning? (Remember that in this course “normalization” is understood in the wide sense and includes the transformations performed by `Normalizer`, `StandardScaler`, etc., in `scikit-learn`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transforming the dataset so that it is compatible with the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Why is normalization of features not essential for the method of Least Squares?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Least Square is not sensitive to the scale of features\n",
    "$$ J(θ) = 1/2m ∑i=1 (hθ(x(i)) − y(i))^2 $$ \n",
    "  - where $hθ$ is the hypothesis function and $x(i)$ is the $ith$ training example\n",
    "- The only term in the cost function that depends on the feature $x$ is $hθ(x(i))$\n",
    "  - The cost function is only sensitive to the relative values of the features, not the absolute values\n",
    "  - Hence, if the features were normalized, only the scale of the features would be changed but not the relative values\n",
    "- Therefore, the function would not be affected and we would get the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Why is normalization of features essential for Ridge Regression and the Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As mentioned before, both Ridge Regression and Lasso regularize the cost function by adding a term to the cost function that penalizes the coefficients for being too large\n",
    "- These methods are designed to prevent overfitting by reducing the magnitude of the coefficients\n",
    "- If the features are not normalized, then the regularization term will penalize the coefficients differenctly depending on the scale of the features\n",
    "- For example, there are 2 features $x_1$ and $x_2$\n",
    "  - $x_1$ is from range 0 to 1\n",
    "  - $x_2$ is in range 0 to 1000 (possibly because of different units)\n",
    "  - In this case, the regularization term will penalize $x_2$ much more than $x_1$ as it is much larger\n",
    "  - This would then over-represent $x_1$ in the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Briefly describe the class `StandardScaler` in `scikit-learn`, paying particular attention to its fit and transform methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `StandardScaler` ensures for each feature, the mean is 0 and the variance is 1\n",
    "- This brings all the features to the same scale\n",
    "- Steps:\n",
    "  - Shift each feature down by its mean\n",
    "  - Divide each feature by its standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "Briefly describe the class `RobustScaler` in `scikit-learn`, paying particular attention to its fit and transform methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because `StandardScaler` uses the mean and variance, there is chance that it may be inaccurate due to outliers\n",
    "- The `RobustScaler` uses the median and quartiles (interquartile range) ignoring outliers\n",
    "- Steps:\n",
    "  - Shift each feature down by its median\n",
    "  - Divide each feature by its interquartile range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "Briefly describe the class `MinMaxScaler` in `scikit-learn`, paying particular attention to its fit and transform methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `MinMax` shifts all the features such that all the feature range is between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "Briefly describe the class `Normalizer` in `scikit-learn`, paying particular attention to its fit and transform methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of normalizing features, samples are normalized instead\n",
    "- Each sample is divided by its Euclidean norm\n",
    "- This means that normalizing the training and test sets not required (automatic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "Give an example of a dataset for which the use of the class Normalizer has a better justification than the use of classes performing normalization of features (such as `StandardScaler`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "Consider the following training set:\n",
    "| Feature 1  | Feature 2  | Label  |\n",
    "| -----------  | -----------  |  -----------  |\n",
    "| -3 | 2 | Male |\n",
    "| 0 | 5 | Female |\n",
    "| 3 | 8 | Male |\n",
    "| 0 | 8 | Male |\n",
    "\n",
    "What is its normalized version, in the sense of `MinMaxScaler`? Apply the same transformation to the test set\n",
    "| Feature 1 | Feature 2 |\n",
    "| ----------- | ----------- |\n",
    "| 1 | -1 |\n",
    "| 0 | 4 |\n",
    "| 2 | 5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Shift all features by the smallest number (smallest feature in training set becomes 0)\n",
    "2. Divide all features by biggest feature (largest feature in training set becomes 1)\n",
    "\n",
    "**Training Set**\n",
    "*Feature 1*\n",
    "| Original | Shift | Shifted | Divide | Divided |\n",
    "| ----------- | ----------- |  ----------- |  ----------- |  ----------- |\n",
    "| -3 | +3 | 0 | ÷6 | 0 |\n",
    "| 0 | +3 | 3 | ÷6 | 1/2 |\n",
    "| 3 | +3 | 6 | ÷6 | 1 |\n",
    "| 0 | +3 | 3 | ÷6 | 1/2 |\n",
    "\n",
    "*Feature 2*\n",
    "| Original | Shift | Shifted | Divide | Divided |\n",
    "| ----------- | ----------- |  ----------- |  ----------- |  ----------- |\n",
    "| 2 | +2 | 0 | ÷6 | 0 |\n",
    "| 5 | +2 | 3 | ÷6 | 1/2 |\n",
    "| 8 | +2 | 6 | ÷6 | 1 |\n",
    "| 8 | +2 | 3 | ÷6 | 1 |\n",
    "\n",
    "**Test Set**\n",
    "- Apply Feature 1 (training set) transformation to Feature 1 (test set)\n",
    "- Same for feature 2\n",
    "\n",
    "*Feature 1*\n",
    "| Original | Shift | Shifted | Divide | Divided |\n",
    "| ----------- | ----------- |  ----------- |  ----------- |  ----------- |\n",
    "| 1 | +3 | 4 | ÷6 | 2/3 |\n",
    "| 0 | +3 | 3 | ÷6 | 1/2 |\n",
    "| 2 | +3 | 5 | ÷6 | 5/6 |\n",
    "\n",
    "*Feature 2*\n",
    "| Original | Shift | Shifted | Divide | Divided |\n",
    "| ----------- | ----------- |  ----------- |  ----------- |  ----------- |\n",
    "| -1 | +2 | -3 | ÷6 | -1/2 |\n",
    "| 4 | +2 | 2 | ÷6 | 1/3 |\n",
    "| 5 | +2 | 3 | ÷6 | 1/2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "For the training set\n",
    "| Feature 1  | Feature 2  | Label  |\n",
    "| -----------  | -----------  |  -----------  |\n",
    "| -10 | 0 | 1.6 |\n",
    "| 10 | 2 | 2.8 |\n",
    "\n",
    "find its normalized version in the sense of `StandardScaler`. Apply the same transformation (emulating the transform method) to the test set\n",
    "| Feature 1 | Feature 2 |\n",
    "| ----------- | ----------- |\n",
    "| -20 | -2 |\n",
    "| 10 | 4 |\n",
    "| 0 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Shift each feature down by its mean\n",
    "2. Divide each feature by its standard deviation\n",
    "\n",
    "**Training Set**\n",
    "*Feature 1*\n",
    "\n",
    "Mean = $\\frac{(-10)+(10)}{2}=0$\n",
    "Standard Deviation = $\\sqrt{\\frac{((-10)-(0))^2+((10)-(0))^2}{2}}=10$\n",
    "\n",
    "| Original | Mean | Shifted | Standard Deviation | Normalized |\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| -10 | -0 | -10 | ÷10 | -1 |\n",
    "| 10 | -0 | 10 | ÷10 | 1 |\n",
    "\n",
    "*Feature 2*\n",
    "\n",
    "Mean = $\\frac{0+2}{2}=1$\n",
    "Standard Deviation = $\\sqrt{\\frac{((0)-(1))^2+((2)-(1))^2}{2}}=1$\n",
    "\n",
    "| Original | Mean | Shifted | Standard Deviation | Normalized |\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| 0 | -1 | -1 | ÷1 | -1 |\n",
    "| 2 | -1 | 1 | ÷1 | 1 |\n",
    "\n",
    "**Test Set**\n",
    "- Apply training sets transformation to test sets transformation corresponding the each feature\n",
    "\n",
    "*Feature 1*\n",
    "| Original | Mean | Shifted | Standard Deviation | Normalized |\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| -20 | -0 | -20 | ÷10 | -2 |\n",
    "| 10 | -0 | 10 | ÷10 | 1 |\n",
    "| 0 | -0 | 0 | ÷10 | 0 |\n",
    "\n",
    "*Feature 2*\n",
    "| Original | Mean | Shifted | Standard Deviation | Normalized |\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| -2 | -1 | -3 | ÷1 | -3 |\n",
    "| 4 | -1 | 3 | ÷1 | 3 |\n",
    "| 0 | -1 | -1 | ÷1 | -1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "Consider the following training set:\n",
    "| Feature 1 | Feature 2 |\n",
    "| ----------- | ----------- |\n",
    "| -3 | 4 |\n",
    "| 4 | 3 |\n",
    "| 4 | 4 |\n",
    "\n",
    "What is its normalized version, in the sense of `Normalizer`? Apply the same transformation to the test set\n",
    "| Feature 1 | Feature 2 |\n",
    "| ----------- | ----------- |\n",
    "| -4 | 3 |\n",
    "| 3 | -3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each sample (row), compute the Euclidean distance and divide\n",
    "\n",
    "**Training Set**\n",
    "| Feature 1 | Feature 2 | Euclidean Distance | Feature 1 Transformed | Feature 2 Transformed |\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| -3 | 4 | $÷\\sqrt{(-3)^2+(4)^2}=5$ | -3/5 | 4/5 |\n",
    "| 4 | 3 | $÷\\sqrt{(4)^2+(3)^2}=5$ | 4/5 | 3/5 |\n",
    "| 4 | 4 | $÷\\sqrt{(4)^2+(4)^2}=4\\sqrt{2}$ | $\\sqrt{2}/2$ | $\\sqrt{2}/2$ |\n",
    "\n",
    "**Test Set**\n",
    "| Feature 1 | Feature 2 | Euclidean Distance | Feature 1 Transformed | Feature 2 Transformed |\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| -4 | 3 | $÷\\sqrt{(-4)^2+(3)^2}=5$ | -4/5 | 3/5 |\n",
    "| 3 | 3 | $÷\\sqrt{(3)^2+(3)^2}=3\\sqrt{2}$ | $\\sqrt{2}/2$ | $\\sqrt{2}/2$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "What is meant by data snooping in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data Snooping** - using test set to develop model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "What is wrong with the following code for data normalization?\n",
    "```py\n",
    "X = MinMaxScaler().fit_transform(boston.data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,boston.target)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the data first instead of applying normalization and then splitting as this can be classed as data snooping\n",
    "```py\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target)\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14\n",
    "Discuss disadvantages of normalizing the training and test sets separately when using classes `StandardScaler`, `RobustScaler`, and `MinMaxScaler`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the test set is normalized separately, then it will not conform with the training set \n",
    "- This means that the model is not able to accurately generalize the data as it is inconsistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15\n",
    "Is it admissible to normalize training and test set separately when using the class `Normalizer`? Explain briefly why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yes, it is possible\n",
    "- This is because instead of each feature being normalized individually, the whole sample (row) is normalized independently\n",
    "- Each sample has its own unique normalization and does not depend on the normalization of another sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16\n",
    "What is wrong with the following code for data normalization?\n",
    "```py\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, boston.target)\n",
    "X_train = MinMaxScaler().fit_transform(X_train)\n",
    "X_test = MinMaxScaler().fit_transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target)\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17\n",
    "Explain the use of a validation set for parameter selection in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Validation Set** - used to estimate how well the model will generalize to new data\n",
    "  - Helps select the model with the best performance and avoid overfitting\n",
    "- This would require some data that has not been using for validation \n",
    "  - However, the test set cannot be used as this is data snooping\n",
    "  - Hence, a new pseudo test set is created out of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18\n",
    "What are disadvantages of the use of the test set for parameter selection (i.e., of choosing the parameters that give the best results on the test set)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The test set should not be available to the model\n",
    "- If it is, then it would be data snooping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 19\n",
    "Explain the use of cross-validation for parameter selection in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splitting data into training set and test set does not always return the same result depending on how the data was split (eg `random_state`)\n",
    "- By this logic, splitting the training set into training set proper and validation set also has the same problems\n",
    "- Hence, after the split, cross validation is used to evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 20\n",
    "How would you perform parameter selection using grid search in a hierarchical manner to improve its computational efficiency?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start with a small and coarse grid (as grid searches are computationally expensive)\n",
    "- Ideally, the values at the centre of the first grid should be significantly better than the values at the boundary \n",
    "  - Less adjustments for the small grid\n",
    "- Then, a finer grid is used to isolate the area of good performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 21\n",
    "(The entries in the table are the accuracy of the algorithm for different values of the parameters.) How suitable was this grid for selecting the optimal values of the two parameters? Explain briefly why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The gaps between gamma and `C` are too small hence the values are too close together\n",
    "  - The grid is too small\n",
    "- A logarithmic scale (0.001, 0.01, 0.1, ...) would be much better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 22\n",
    "Answer Question 21 for the following grid for parameters A and B:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Much better as there is greater range of values as the size of the grid has been increased by using a logarithmic scale\n",
    "- This can still be improved as the values on the bottom right edge (A -> 100, B -> 100) are higher in accuracy \n",
    "- Those values should be in the middle of the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 23\n",
    "Answer Question 21 for the following grid for parameters A and B:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This grid is good because the optimal values are in the middle of the grid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 24\n",
    "List three desiderata for the method of inductive conformal prediction. Which of them are satisfied automatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The method should allow for accurate prediction of new data.\n",
    "2. The method should be computationally efficient.\n",
    "3. The method should be able to handle non-linear data.\n",
    "\n",
    "1. and 2. are satisfied automatically, while 3. is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 25\n",
    "Briefly explain why conformal prediction is not feasible in combination with feature normalization and parameter selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When processing is required, then the conformal prediction will need be redone for each test sample and for each potential label for it (to achieve guaranteed validity)\n",
    "- True for parameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 26\n",
    "Compare and contrast conformal prediction and inductive conformal prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conformal Prediction**\n",
    "- More computationally intensive\n",
    "- Requires the assumption of exchangeability\n",
    "- More accurate predictions\n",
    "\n",
    "**Inductive Conformal Prediction**\n",
    "- Computationally efficient\n",
    "- Automatically valid under the IID assumption \n",
    "- May suffer a drop in predictive efficiency\n",
    "\n",
    "> Inductive conformal prediction relies on the assumption of IID data, while conformal prediction relies on the assumption of exchangeability. Conformal prediction is also more computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 27\n",
    "Compare and contrast conformal prediction and cross-conformal prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conformal Prediction**\n",
    "- More computationally intensive\n",
    "- Requires the assumption of exchangeability\n",
    "- More accurate predictions\n",
    "\n",
    "**Cross-Conformal Prediction**\n",
    "- Less computationally intensive\n",
    "- Does not require the assumption of exchangeability\n",
    "- Less accurate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 28\n",
    "Compare and contrast inductive conformal prediction and cross-conformal prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inductive Conformal Prediction**\n",
    "- More computationally intensive\n",
    "- Requires the assumption of exchangeability\n",
    "- More accurate predictions\n",
    "- More robust to non-stationarity\n",
    "\n",
    "**Cross-Conformal Prediction**:\n",
    "- Less computationally intensive\n",
    "- Does not require the assumption of exchangeability\n",
    "- Less accurate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 29\n",
    "What is an inductive conformity measure? Define the inductive conformal predictor based on a given inductive conformity measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An inductive conformity measure is a function that quantifies the degree to which a prediction is consistent with a set of data\n",
    "- The inductive conformal predictor is a function that predicts the value of a new data point based on the inductive conformity measure\n",
    "---\n",
    "- Let $C$ be an inductive conformity measure, and let $x$ be a new data point. Then the inductive conformal predictor is given by:\n",
    "$$p(x) = C(x, D)$$\n",
    "  - where $D$ is the set of data points used to train the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 30\n",
    "What is an inductive nonconformity measure? Define the inductive conformal predictor based on a given inductive nonconformity measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An inductive nonconformity measure is a function that quantifies the degree to which a prediction is inconsistent with a set of data\n",
    "- The inductive conformal predictor is a function that predicts the value of a new data point based on the inductive nonconformity measure\n",
    "\n",
    "---\n",
    "Let $C$ be an inductive nonconformity measure, and let $x$ be a new data point. Then the inductive conformal predictor is given by:\n",
    "$$p(x) = C(x, D)$$\n",
    "  - where $D$ is the set of data points used to train the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 31\n",
    "Give three examples of inductive nonconformity measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Incidence of nonconforming items\n",
    "2. Severity of nonconformities\n",
    "3. Rate of occurrence of nonconformities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 32\n",
    "Give two examples of inductive conformity measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Percentage of conforming items\n",
    "2. Percentage of conforming batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 33\n",
    "In the context of inductive conformal prediction, what is the minimal possible p-value for a training set proper of size $n−m$ and calibration set of size $m$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{m+1}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('anaconda-pOKeClin-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d6ff4c17b0f926f3435e2604897e7630e7b57ea2fee9d98b1dac9eecef6094c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
